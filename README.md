# ATSynKD: Using Attention Transfer and Synthetic Data for Unsupervised Few-Sample Feature-Based Knowledge Distillation

Project code and experimental results for our AI535 Deep Learning Final Project.

Based on the works:
* [Paying More Attention to Attention](https://arxiv.org/pdf/1612.03928)
* [Few Sample Knowledge Distillation for Efficient Network Compression](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.html)
* [Black-Box Few-Shot Knowledge Distillation](https://arxiv.org/abs/2207.12106)
* [Knowledge Distillation from Internal Representations
](https://cdn.aaai.org/ojs/6229/6229-13-9454-1-10-20200516.pdf)

---
2025 &copy; Colin Pannikkat and Ajinkya Gokule
